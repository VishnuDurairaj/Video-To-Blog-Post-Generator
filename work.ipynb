{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "token = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "\n",
    "def save_video_frames(video_path, output_folder):\n",
    "    # Load the video\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    \n",
    "    # Check if video loaded successfully\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Could not open video.\")\n",
    "        return\n",
    "    \n",
    "    # Create the output folder if it does not exist\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    # Get total frame count\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    print(f\"Total frames: {total_frames}\")\n",
    "\n",
    "    frame_index = 0\n",
    "    \n",
    "    # Loop through all frames\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        \n",
    "        # Break the loop if no frame is returned (end of video)\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Save frame as an image\n",
    "        frame_filename = os.path.join(output_folder, f\"frame_{frame_index:04d}.jpg\")\n",
    "        cv2.imwrite(frame_filename, frame)\n",
    "        \n",
    "        print(f\"Saved {frame_filename}\")\n",
    "        \n",
    "        frame_index += 1\n",
    "\n",
    "    # Release the video capture object\n",
    "    cap.release()\n",
    "    print(\"All frames saved successfully.\")\n",
    "\n",
    "# Usage\n",
    "video_path = \"test_video1.mp4\"  # Path to the input video\n",
    "output_folder = \"output_frames\"  # Folder to save frames\n",
    "# save_video_frames(video_path, output_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'calculate_image_similarity' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 27\u001b[0m\n\u001b[0;32m     25\u001b[0m img1_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_frames/frame_0619.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     26\u001b[0m img2_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_frames/frame_0621.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 27\u001b[0m similarity_score \u001b[38;5;241m=\u001b[39m \u001b[43mcalculate_image_similarity\u001b[49m(img1_path, img2_path)\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSimilarity score: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msimilarity_score\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'calculate_image_similarity' is not defined"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "def calculate_pixel_similarity(img1_path, img2_path):\n",
    "    # Load the images in grayscale\n",
    "    img1 = cv2.imread(img1_path)\n",
    "    img2 = cv2.imread(img2_path)\n",
    "    \n",
    "    # Check if images loaded correctly\n",
    "    if img1 is None or img2 is None:\n",
    "        print(\"Error: One or both images could not be loaded.\")\n",
    "        return None\n",
    "\n",
    "    # Ensure the images are the same size\n",
    "    if img1.shape != img2.shape:\n",
    "        print(\"Error: Images must have the same dimensions.\")\n",
    "        return None\n",
    "\n",
    "    # Calculate pixel-wise difference\n",
    "    similarity = cv2.norm(img1, img2, cv2.NORM_L2) / (img1.size)\n",
    "    similarity_score = 1 - similarity  # Normalize similarity to 0-1 scale\n",
    "\n",
    "    return similarity\n",
    "\n",
    "# Usage example\n",
    "img1_path = r\"output_frames/frame_0619.jpg\"\n",
    "img2_path = r\"output_frames/frame_0621.jpg\"\n",
    "similarity_score = calculate_image_similarity(img1_path, img2_path)\n",
    "print(f\"Similarity score: {similarity_score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SSIM Similarity score: 0.9758\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "\n",
    "def calculate_ssim_similarity(img1, img2):\n",
    "\n",
    "    # Check if images loaded correctly\n",
    "    if img1 is None or img2 is None:\n",
    "        print(\"Error: One or both images could not be loaded.\")\n",
    "        return None\n",
    "\n",
    "    # Ensure the images are the same size\n",
    "    if img1.shape != img2.shape:\n",
    "        print(\"Error: Images must have the same dimensions.\")\n",
    "        return None\n",
    "\n",
    "    # Calculate SSIM between the two images\n",
    "    similarity_score, _ = ssim(img1, img2, full=True)\n",
    "\n",
    "    return similarity_score\n",
    "\n",
    "# Usage example\n",
    "img1_path = r\"output_frames/frame_0619.jpg\"\n",
    "img2_path = r\"output_frames/frame_0621.jpg\"\n",
    "similarity_score = calculate_ssim_similarity(img1_path, img2_path)\n",
    "print(f\"SSIM Similarity score: {similarity_score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Histogram Similarity score: -0.0008\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "def calculate_histogram_similarity(img1_path, img2_path):\n",
    "    # Load the images in color\n",
    "    img1 = cv2.imread(img1_path)\n",
    "    img2 = cv2.imread(img2_path)\n",
    "    \n",
    "    # Check if images loaded correctly\n",
    "    if img1 is None or img2 is None:\n",
    "        print(\"Error: One or both images could not be loaded.\")\n",
    "        return None\n",
    "\n",
    "    # Resize images to the same size if they differ\n",
    "    if img1.shape != img2.shape:\n",
    "        img2 = cv2.resize(img2, (img1.shape[1], img1.shape[0]))\n",
    "\n",
    "    # Convert images to HSV color space\n",
    "    img1_hsv = cv2.cvtColor(img1, cv2.COLOR_BGR2HSV)\n",
    "    img2_hsv = cv2.cvtColor(img2, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "    # Calculate histograms and normalize them\n",
    "    hist_img1 = cv2.calcHist([img1_hsv], [0, 1], None, [50, 60], [0, 180, 0, 256])\n",
    "    hist_img2 = cv2.calcHist([img2_hsv], [0, 1], None, [50, 60], [0, 180, 0, 256])\n",
    "    cv2.normalize(hist_img1, hist_img1, 0, 1, cv2.NORM_MINMAX)\n",
    "    cv2.normalize(hist_img2, hist_img2, 0, 1, cv2.NORM_MINMAX)\n",
    "\n",
    "    # Compare histograms using correlation\n",
    "    similarity_score = cv2.compareHist(hist_img1, hist_img2, cv2.HISTCMP_CORREL)\n",
    "\n",
    "    return similarity_score\n",
    "\n",
    "img1_path = r\"output_frames/frame_0620.jpg\"\n",
    "img2_path = r\"output_frames/frame_0067.jpg\"\n",
    "similarity_score = calculate_histogram_similarity(img1_path, img2_path)\n",
    "print(f\"Histogram Similarity score: {similarity_score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame rate: 60.0 FPS\n",
      "Total frames extracted: 2581\n",
      "Extracted 2581 frames.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "def extract_video_frames(video_path, frame_interval=1):\n",
    "    \"\"\"\n",
    "    Extracts frames from a video at the specified interval.\n",
    "\n",
    "    Parameters:\n",
    "    - video_path (str): Path to the video file.\n",
    "    - frame_interval (int): Interval at which frames are saved (e.g., 1 = every frame, 2 = every other frame).\n",
    "    \n",
    "    Returns:\n",
    "    - frames (list): List of frames extracted from the video.\n",
    "    \"\"\"\n",
    "    # Load the video\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Could not open video.\")\n",
    "        return []\n",
    "\n",
    "    # Retrieve frame rate to process frames efficiently\n",
    "    frame_rate = cap.get(cv2.CAP_PROP_FPS)\n",
    "    print(f\"Frame rate: {frame_rate} FPS\")\n",
    "\n",
    "    frames = []\n",
    "    frame_index = 0\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        # Break the loop if there are no frames left to read\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Process only every `frame_interval` frame\n",
    "        if frame_index % frame_interval == 0:\n",
    "            frames.append(frame)\n",
    "\n",
    "        frame_index += 1\n",
    "\n",
    "    cap.release()\n",
    "    print(f\"Total frames extracted: {len(frames)}\")\n",
    "\n",
    "    return frames\n",
    "\n",
    "# Usage example\n",
    "video_path = \"test_video1.mp4\"  # Path to your video file\n",
    "frame_interval = 1  # Extract every second frame\n",
    "frames = extract_video_frames(video_path, frame_interval)\n",
    "print(f\"Extracted {len(frames)} frames.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cv2\n",
    "# import numpy as np\n",
    "\n",
    "# from skimage.metrics import structural_similarity as ssim\n",
    "\n",
    "# def find_stable_frames(frames, frame_rate, similarity_threshold=0.95, stable_duration=5):\n",
    "#     \"\"\"\n",
    "#     Identify stable frames in a list of video frames.\n",
    "\n",
    "#     Parameters:\n",
    "#     - frames (list): List of frames extracted from a video.\n",
    "#     - frame_rate (float): Frame rate of the video.\n",
    "#     - similarity_threshold (float): SSIM similarity threshold for consecutive frames to be considered stable.\n",
    "#     - stable_duration (float): Minimum duration in seconds for frames to be considered stable.\n",
    "\n",
    "#     Returns:\n",
    "#     - stable_frames (list): List of tuples with start and end timestamps for stable frames.\n",
    "#     \"\"\"\n",
    "#     # Calculate the required consecutive stable frame count\n",
    "#     required_stable_frames = int(stable_duration * frame_rate)\n",
    "#     stable_frames = []\n",
    "    \n",
    "#     stable_frame_count = 0\n",
    "#     start_time = None\n",
    "\n",
    "#     start_index = None\n",
    "#     end_index = None\n",
    "\n",
    "#     # Iterate through consecutive frames and calculate SSIM\n",
    "#     for i in range(1, len(frames)):\n",
    "        \n",
    "#         if i%100==0:\n",
    "#             print(\"processing : \",i)\n",
    "#         # Convert frames to grayscale\n",
    "#         frame1_gray = cv2.cvtColor(frames[i - 1], cv2.COLOR_BGR2GRAY)\n",
    "#         frame2_gray = cv2.cvtColor(frames[i], cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "#         # Calculate SSIM similarity between consecutive frames\n",
    "#         similarity_score, _ = ssim(frame1_gray, frame2_gray, full=True)\n",
    "        \n",
    "#         # Check if the similarity meets the threshold\n",
    "#         if similarity_score >= similarity_threshold:\n",
    "#             # Start tracking stable sequence\n",
    "#             if stable_frame_count == 0:\n",
    "#                 start_time = (i - 1) / frame_rate  # Start timestamp of stable sequence\n",
    "#             if not start_index:\n",
    "#                 start_index = i\n",
    "#             stable_frame_count += 1\n",
    "#         else:\n",
    "#             end_index = i-1\n",
    "#             # End stable sequence if similarity falls below threshold\n",
    "#             if stable_frame_count >= required_stable_frames:\n",
    "#                 end_time = (i - 1) / frame_rate  # End timestamp of stable sequence\n",
    "#                 if end_index> start_index:\n",
    "#                     stable_frames.append([frames[np.random.randint(start_index+1,end_index-1)],(start_time, end_time)])\n",
    "#                 else:\n",
    "#                     stable_frames.append([frames[start_index],(start_time, end_time)])\n",
    "#             # Reset stable frame count\n",
    "#             stable_frame_count = 0\n",
    "#             start_index = None\n",
    "\n",
    "#     # Handle case where last sequence is stable\n",
    "#     if stable_frame_count >= required_stable_frames:\n",
    "#         end_index = i-1\n",
    "#         end_time = (len(frames) - 1) / frame_rate\n",
    "#         stable_frames.append([frames[start_index],(start_time, end_time)])\n",
    "\n",
    "#     return stable_frames\n",
    "\n",
    "# # Example Usage\n",
    "# # Assuming 'frames' is the list of frames extracted from the video\n",
    "# frame_rate = 60  # Replace with the actual frame rate of the video\n",
    "# stable_frames = find_stable_frames(frames, frame_rate, similarity_threshold=0.99, stable_duration=5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***************** Extracting Frames From the Video *****************************\n",
      "Frame rate: 30.0 FPS\n",
      "Total frames extracted: 1698\n",
      "Extracted 1698 frames.\n",
      "***************** Finding The Stable Frames From The Video **********************\n",
      "processing : 100/1698\n",
      "processing : 200/1698\n",
      "processing : 300/1698\n",
      "Stable Frame Starts 4.166666666666667 and ends at 12.566666666666666 253 Required :  90\n",
      "processing : 400/1698\n",
      "processing : 500/1698\n",
      "Stable Frame Starts 12.833333333333334 and ends at 19.266666666666666 194 Required :  90\n",
      "processing : 600/1698\n",
      "processing : 700/1698\n",
      "processing : 800/1698\n",
      "processing : 900/1698\n",
      "processing : 1000/1698\n",
      "Stable Frame Starts 19.766666666666666 and ends at 35.9 485 Required :  90\n",
      "processing : 1100/1698\n",
      "processing : 1200/1698\n",
      "Stable Frame Starts 36.3 and ends at 41.5 157 Required :  90\n",
      "processing : 1300/1698\n",
      "processing : 1400/1698\n",
      "processing : 1500/1698\n",
      "Stable Frame Starts 42.0 and ends at 51.5 286 Required :  90\n",
      "processing : 1600/1698\n",
      "MoviePy - Writing audio in audio.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Audio Extracted Successfully....\n",
      "[0.56s -> 3.48s]  Let's learn k-means algorithm in just one minute.\n",
      "[3.48s -> 9.16s]  In step 1, we have to decide how many clusters we want, which is denoted as k in k-means\n",
      "[9.16s -> 10.16s]  clustering.\n",
      "[10.16s -> 13.04s]  Let's say we are expecting 3 clusters in this case.\n",
      "[13.04s -> 17.04s]  Then in step 2, we have to randomly initialize 3 data points.\n",
      "[17.04s -> 19.84s]  These 3 data points are called centrites.\n",
      "[19.84s -> 24.96s]  And in step 3, we have to assign each data point to its nearest centroid.\n",
      "[24.96s -> 30.16s]  I mean take a data point, calculate distance from all three centroid and assign the data\n",
      "[30.16s -> 32.02s]  point to its nearest centroid.\n",
      "[32.02s -> 36.22s]  We have to do the same thing for each and every data point.\n",
      "[36.22s -> 41.48s]  And in step 4, we have to update the centroid by taking mean of each group.\n",
      "[41.48s -> 46.64s]  And in step 5, we have to repeat step 3 and 4 until the difference between previous centroid\n",
      "[46.64s -> 48.84s]  and current centroid becomes zero.\n",
      "[48.84s -> 51.56s]  Finally, this is how your cluster would look like.\n",
      "Blog post saved at: blog_post\\centered_blog_post.html\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Blog Created Successfully.......'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from moviepy.editor import VideoFileClip\n",
    "from faster_whisper import WhisperModel\n",
    "from utils.helper import create_blog_post\n",
    "import ffmpeg\n",
    "import soundfile as sf\n",
    "import os\n",
    "\n",
    "\n",
    "class VideoToBlog:\n",
    "\n",
    "    def __init__(self,video_path,title,frame_interval=1,similarity_threshold=0.99,stable_duration_sec=5,transcription_model=\"large-v3\",device=\"cpu\",compute_type=\"int8\"):\n",
    "        self.video_path = video_path\n",
    "        self.title=title\n",
    "        self.frame_interval = frame_interval\n",
    "        self.similarity_threshold = similarity_threshold\n",
    "        self.stable_duration_sec = stable_duration_sec\n",
    "        os.makedirs(\"stable_frames\",exist_ok=True)\n",
    "        self.__transcription_model = WhisperModel(transcription_model, device=device, compute_type=compute_type)\n",
    "\n",
    "    def extract_video_frames(self):\n",
    "        \"\"\"\n",
    "        Extracts frames from a video at the specified interval.\n",
    "\n",
    "        Parameters:\n",
    "        - video_path (str): Path to the video file.\n",
    "        - frame_interval (int): Interval at which frames are saved (e.g., 1 = every frame, 2 = every other frame).\n",
    "        \n",
    "        Returns:\n",
    "        - frames (list): List of frames extracted from the video.\n",
    "        \"\"\"\n",
    "        # Load the video\n",
    "        cap = cv2.VideoCapture(self.video_path)\n",
    "        if not cap.isOpened():\n",
    "            print(\"Error: Could not open video.\")\n",
    "            return []\n",
    "\n",
    "        # Retrieve frame rate to process frames efficiently\n",
    "        frame_rate = cap.get(cv2.CAP_PROP_FPS)\n",
    "        print(f\"Frame rate: {frame_rate} FPS\")\n",
    "\n",
    "        frames = []\n",
    "        frame_index = 0\n",
    "\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "\n",
    "            # Break the loop if there are no frames left to read\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "            # Process only every `frame_interval` frame\n",
    "            if frame_index % self.frame_interval == 0:\n",
    "                frames.append(frame)\n",
    "\n",
    "            frame_index += 1\n",
    "\n",
    "        cap.release()\n",
    "        print(f\"Total frames extracted: {len(frames)}\")\n",
    "        return frames, frame_rate\n",
    "\n",
    "    def find_stable_frames(self,frames, frame_rate):\n",
    "        \"\"\"\n",
    "        Identify stable frames in a list of video frames.\n",
    "\n",
    "        Parameters:\n",
    "        - frames (list): List of frames extracted from a video.\n",
    "        - frame_rate (float): Frame rate of the video.\n",
    "        - similarity_threshold (float): SSIM similarity threshold for consecutive frames to be considered stable.\n",
    "        - stable_duration (float): Minimum duration in seconds for frames to be considered stable.\n",
    "\n",
    "        Returns:\n",
    "        - stable_frames (list): List of tuples with start and end timestamps for stable frames.\n",
    "        \"\"\"\n",
    "        # Calculate the required consecutive stable frame count\n",
    "        required_stable_frames = int(self.stable_duration_sec * frame_rate)\n",
    "        stable_frames = []\n",
    "        stable_frames_count=0\n",
    "        start_and_end = []\n",
    "        start_ind = None\n",
    "        end_ind = None\n",
    "        start_time = None\n",
    "        end_time = None\n",
    "\n",
    "        current_stable_frames = []\n",
    "\n",
    "        for i in range(1, len(frames)):\n",
    "\n",
    "            if i%100==0:\n",
    "\n",
    "                print(f\"processing : {i}/{len(frames)}\")\n",
    "\n",
    "            # Convert frames to grayscale\n",
    "            frame1_gray = cv2.cvtColor(frames[i - 1], cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "            frame2_gray = cv2.cvtColor(frames[i], cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "            similarity_score, _ = ssim(frame1_gray, frame2_gray, full=True)\n",
    "\n",
    "            if similarity_score>=self.similarity_threshold:\n",
    "\n",
    "                if not start_time:\n",
    "\n",
    "                    start_ind = i-1\n",
    "\n",
    "                    stable_frames_count +=1\n",
    "\n",
    "                    if i==0:\n",
    "\n",
    "                        start_time=0.0001\n",
    "\n",
    "                    else:\n",
    "\n",
    "                        start_time = i/frame_rate\n",
    "\n",
    "                    current_stable_frames.append(frames[i])\n",
    "\n",
    "                else:\n",
    "\n",
    "                    current_stable_frames.append(frames[i])\n",
    "\n",
    "            else:\n",
    "\n",
    "                if len(current_stable_frames)>=required_stable_frames:\n",
    "\n",
    "                    end_ind = i-1\n",
    "\n",
    "                    end_time = (i-1)/frame_rate\n",
    "\n",
    "                    print(f\"Stable Frame Starts {start_time} and ends at {end_time}\")\n",
    "\n",
    "                    stable_frames.append(frames[i-1])\n",
    "\n",
    "                    start_and_end.append((start_time,end_time))\n",
    "\n",
    "                    cv2.imwrite(f\"stable_frames/image_{stable_frames_count}.jpg\",frames[i-1])\n",
    "\n",
    "                else:\n",
    "\n",
    "                    pass\n",
    "                    \n",
    "                    # print(\"Skipping.. :\",len(current_stable_frames),\"Required : \",required_stable_frames)\n",
    "\n",
    "                start_time,end_time, start_ind, end_ind, current_stable_frames = None,None,None,None,[]\n",
    "\n",
    "        if len(current_stable_frames)>=required_stable_frames:\n",
    "\n",
    "            end_ind = i=i\n",
    "\n",
    "            end_time = (i-1)/frame_rate\n",
    "\n",
    "            stable_frames.append(frames[np.random.randint(start_ind+1,end_ind-1)])\n",
    "\n",
    "            start_and_end.append((start_time,end_time))\n",
    "\n",
    "        gc.collect()\n",
    "        \n",
    "        return stable_frames, start_and_end\n",
    "    \n",
    "    def extract_audio(self,audio_output_path=\"audio.mp3\"):\n",
    "        \"\"\"\n",
    "        Extracts audio from a video file and saves it to the specified output path.\n",
    "\n",
    "        :param video_path: Path to the input video file.\n",
    "        :param audio_output_path: Path to save the extracted audio file.\n",
    "        \"\"\"\n",
    "        # Load the video file\n",
    "        video = VideoFileClip(self.video_path)\n",
    "        \n",
    "        # Extract audio\n",
    "        audio = video.audio\n",
    "        \n",
    "        # Write audio to file\n",
    "        audio.write_audiofile(audio_output_path)\n",
    "        \n",
    "        # Close the video file\n",
    "        video.close()\n",
    "\n",
    "        print(\"Audio Extracted Successfully....\")\n",
    "\n",
    "    def split_audio_with_ffmpeg(self,audio_path=\"audio_chunk.wav\", timestamps=[], output_dir=\"audio_segments\"):\n",
    "        \"\"\"\n",
    "        Splits an audio file based on given timestamps using FFmpeg and saves each segment.\n",
    "\n",
    "        Parameters:\n",
    "        - audio_path (str): Path to the original audio file.\n",
    "        - timestamps (list of tuples): List of (start, end) timestamps in seconds for splitting.\n",
    "        - output_dir (str): Directory to store split audio segments.\n",
    "\n",
    "        Returns:\n",
    "        - List of paths to the saved audio segments.\n",
    "        \"\"\"\n",
    "        # Create the output directory if it doesn't exist\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "        transcriptions = []\n",
    "\n",
    "        for i, (_, end) in enumerate(timestamps):\n",
    "\n",
    "            if not len(transcriptions):\n",
    "                start = 0\n",
    "\n",
    "            # Define the output path for the segment\n",
    "            segment_path = os.path.join(output_dir, audio_path)\n",
    "\n",
    "            if len(timestamps)==i:\n",
    "\n",
    "                (\n",
    "                ffmpeg\n",
    "                .input(audio_path, ss=start,)\n",
    "                .output(segment_path)\n",
    "                .run(quiet=True, overwrite_output=True)\n",
    "                )\n",
    "            else:\n",
    "                (\n",
    "                    ffmpeg\n",
    "                    .input(audio_path, ss=start, to=end)\n",
    "                    .output(segment_path)\n",
    "                    .run(quiet=True, overwrite_output=True)\n",
    "                )\n",
    "\n",
    "            text = [i[2] for i in self.transcribe_audio(segment_path)]\n",
    "\n",
    "            final_text = \"\\n\".join(text) if len(text)>1 else text[0]\n",
    "\n",
    "            transcriptions.append(final_text)\n",
    "\n",
    "            start = end\n",
    "\n",
    "        return transcriptions\n",
    "\n",
    "            \n",
    "    def transcribe_audio(self,file_path=\"audio.mp3\"):\n",
    "\n",
    "        segments, info = self.__transcription_model.transcribe(file_path, beam_size=5, language=\"en\", condition_on_previous_text=False)\n",
    "        \n",
    "        transcripton = []\n",
    "\n",
    "        for segment in segments:\n",
    "            \n",
    "            print(\"[%.2fs -> %.2fs] %s\" % (segment.start, segment.end, segment.text))\n",
    "\n",
    "            transcripton.append((segment.start, segment.end, segment.text))\n",
    "\n",
    "        return transcripton\n",
    "\n",
    "\n",
    "    def map_frames_and_transcription(self,start_and_end_frame,transcripton):\n",
    "        \n",
    "        final_transcription = []\n",
    "\n",
    "        starting_list = [i[0] for i in transcripton]\n",
    "        ending_list = [i[1] for i in transcripton]\n",
    "        text_list = [i[2] for i in transcripton]\n",
    "\n",
    "        start = 0\n",
    "\n",
    "        for _, end in start_and_end_frame:\n",
    "\n",
    "            # Find closest index in starting_list to start\n",
    "            closest_start_index = min(enumerate(starting_list), key=lambda x: abs(x[1] - start))[0]\n",
    "\n",
    "            # Find closest index in ending_list to end\n",
    "            closest_end_index = min(enumerate(ending_list), key=lambda x: abs(x[1] - end))[0]\n",
    "\n",
    "            start = ending_list[closest_end_index]\n",
    "\n",
    "            if not len(final_transcription):\n",
    "\n",
    "                closest_start_index = 0\n",
    "\n",
    "            final_transcription.append(\"\\n\".join(text_list[closest_start_index:closest_end_index+1]).strip())\n",
    "\n",
    "        return final_transcription\n",
    "\n",
    "\n",
    "    def get_frames_and_time(self):\n",
    "\n",
    "        print(\"***************** Extracting Frames From the Video *****************************\")\n",
    "        frames,frame_rate = self.extract_video_frames()\n",
    "        print(f\"Extracted {len(frames)} frames.\")\n",
    "        print(\"***************** Finding The Stable Frames From The Video **********************\")\n",
    "        stable_frames, start_and_end_frame = self.find_stable_frames(frames, frame_rate)\n",
    "        # print(\"***************** Transcribing The Audio ****************************\")\n",
    "        self.extract_audio()\n",
    "        transcripton = self.transcribe_audio()\n",
    "        final_text = self.map_frames_and_transcription(start_and_end_frame,transcripton)\n",
    "        # transcripton = self.split_audio_with_ffmpeg(\"audio.mp3\",start_and_end_frame)\n",
    "        create_blog_post(self.title,stable_frames,final_text)\n",
    "        return \"Blog Created Successfully.......\"\n",
    "        # return start_and_end_frame,transcripton\n",
    "        \n",
    "video_path = r\"samples\\videoplayback.mp4\"\n",
    "\n",
    "model = VideoToBlog(video_path,\"K-Means Clustering\",stable_duration_sec=3)\n",
    "\n",
    "model.get_frames_and_time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(5.733333333333333, 9.733333333333333),\n",
       " (10.2, 14.0),\n",
       " (15.866666666666667, 25.933333333333334),\n",
       " (26.166666666666668, 38.3),\n",
       " (38.43333333333333, 43.03333333333333),\n",
       " (43.333333333333336, 47.1),\n",
       " (48.43333333333333, 53.3),\n",
       " (55.13333333333333, 58.7)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_and_end_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\" In this video, I'm going to teach you how you can select features using recursive feature\\n elimination.\\n Let's say here we have five features and we need only top three features.\",\n",
       " ' To build a machine learning model, the idea is take all the features',\n",
       " \" build a model and drop the feature which contributes very less to the model.\\n But how do we know which feature contributes less to the model?\\n Well, if you're building a regression model,\",\n",
       " \" then you can use coefficients to make decision.\\n You can drop the feature which has least coefficient value.\\n Or if you are building a decision tree, there you can use feature importance.\\n Let's say,\",\n",
       " ' In this case, feature 4 has very less feature importance, then we have',\n",
       " ' drop this feature and we have to continue the process take the',\n",
       " ' mining features build a model and again look at the feature which has least feature importance and',\n",
       " ' drop that feature we have to continue this process until we get expected number of features']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transcripton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from utils.video2blog import VideoToBlog\n",
    "\n",
    "video_path = \"test_video1.mp4\"\n",
    "\n",
    "model = VideoToBlog(video_path,\"K-Means\",stable_duration_sec=3)\n",
    "\n",
    "# model.get_frames_and_time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(4.166666666666667, 12.566666666666666),\n",
       " (12.833333333333334, 19.266666666666666),\n",
       " (19.766666666666666, 35.9),\n",
       " (36.3, 41.5),\n",
       " (42.0, 51.5)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_and_end_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "videoqa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
